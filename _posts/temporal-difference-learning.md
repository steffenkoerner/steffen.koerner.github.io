---
layout: post
title:  "Temporal Difference Learning"
date:   2022-02-11
categories: Reinforcement Learning
---

# Temporal Difference Learning (TD Learning)
Temporal Difference Learning (TD Learning) solves the same problem as the Monte Carlo Methods. Where MC needs a full episode, TD methods update their estimates based on estimates of other states. Thus, there is no need to wait until the end of a episode.
Updating an estimate without waiting for the end is called bootstrapping. Maybe it still sounds a bit vague. Let's save the our
gridworld with the TD method.


# +++++++++++++++ Unordered Things


Q Learning

Bellmann Equation

Temporal Difference Learning
Monte Carlo Methods
SARSA
On Policy Learning 
Off Policy Learning

The connection between Monte Carlo and TD Learning (n-step TD Learning)

++++ Think about how to create a graph by code. Then there is no need to drag and drop and adapt the things

