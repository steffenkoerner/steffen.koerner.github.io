---
layout: post
title:  "Policy Gradient"
date:   2022-03-11
categories: Deep Reinforcement Learning
---

Why policy based methods 

Simplicity: Policy-based methods directly get to the problem at hand (estimating the optimal policy), without having to store a bunch of additional data (i.e., the action values) that may not be useful.
Stochastic policies: Unlike value-based methods, policy-based methods can learn true stochastic policies.
Continuous action spaces: Policy-based methods are well-suited for continuous action spaces.

See stochastic policy png as example where a value based approach is not very good.


How do the policy based approaches work?
Instead of mapping a state to an actions state value we now map states to actions. The problem is that we now can't use TD Learning anymore. Before we compared the state action value of the current state with the reward plus the maximum action state value of the next state. The goal was to minimize this error. But now we don't have these estimated values. Thus we need to collect the trjactory for the agent.

$$U(\phi) = P(\tau| \phi) R(\tau)$$

Our goal is to maximize the expected return U. One way to do that is by gradient ascent. This is ver closely related to the more famous gradient descent. There are only two differences 

Gradient ascent tries to find the maximum while gradient descent is searching for the minimum.
Gradient ascent steps in the direction of the gradient while gradient descent steps in the direction of the negative gradient

Thus, our parameter update looks as following:

$$\phi \leftarrow \phi + \alpha \nabla_{\phi} U(\phi)$$

where $$\alpha$$ is the step size. The step size decreases and it should be chosen such that $$\lim\limits_{\alpha \to \infty} = 0$$